# Mixtral 파인튜닝

## 1. 알고리즘 설명

Mixtral은 Mixtral of Experts(MoE) 아키텍처 기반의 고성능 오픈소스 언어모델로, 여러 전문가 모듈을 활용해 효율적이고 확장성 높은 파인튜닝이 가능합니다.

## 2. 파라미터 설명

| 파라미터 | 설명 | 일반적인 값 |
|-----------|------|------------|
| num_experts | 전문가 수 | 8, 16, 32 |
| top_k | 활성화할 전문가 수 | 2, 4 |
| learning_rate | 학습률 | 2e-5, 5e-5 |
| batch_size | 배치 크기 | 8, 16, 32 |

## 3. 주요 모델에 대한 GPU 사양

| 모델명 | 모델 크기 | 최소 GPU 사양 | 권장 GPU 사양 |
|--------|-----------|--------------|--------------|
| Mixtral-8x7B | 46B | VRAM 40GB (A100) | VRAM 80GB (A100) |
| Mixtral-8x22B | 176B | VRAM 80GB (A100) | VRAM 160GB (H100) |

## 4. 핸즈온 Example

[이 섹션에는 실제 파인튜닝에 사용할 수 있는 코드 예제 또는 실습 예시를 추가합니다.]
