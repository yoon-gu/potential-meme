# MoE(Mixture of Experts) 파인튜닝

## 1. 알고리즘 설명

MoE(Mixture of Experts)는 여러 개의 전문가(서브모델) 중 일부만 활성화하여 연산 효율성을 높이고, 모델의 용량을 확장하는 파인튜닝 방법입니다. 각 입력에 따라 다른 전문가가 선택되어 처리됩니다.

## 2. 파라미터 설명

| 파라미터 | 설명 | 일반적인 값 |
|-----------|------|------------|
| num_experts | 전문가 수 | 4, 8, 16 |
| top_k | 활성화할 전문가 수 | 1, 2 |
| gate_noise | 게이트 노이즈 | 0.1, 0.5 |
| learning_rate | 학습률 | 1e-4, 5e-5 |

## 3. 주요 모델에 대한 GPU 사양

| 모델명 | 모델 크기 | 최소 GPU 사양 | 권장 GPU 사양 |
|--------|-----------|--------------|--------------|
| Switch Transformer | 1.6B | VRAM 8GB (2080) | VRAM 16GB (3090) |
| GLaM | 1.2T | VRAM 24GB (A6000) | VRAM 40GB (A100) |
| MoE-Llama | 7B | VRAM 16GB (3090) | VRAM 24GB (4090) |

## 4. 핸즈온 Example

[이 섹션에는 실제 파인튜닝에 사용할 수 있는 코드 예제 또는 실습 예시를 추가합니다.]
