# LoRA 파인튜닝

## 1. 알고리즘 설명

LoRA(Low-Rank Adaptation)는 대형 언어 모델의 일부 가중치에만 저랭크 행렬을 추가로 학습시켜 파라미터 효율적으로 파인튜닝하는 방법입니다. 기존 모델의 대부분의 파라미터는 고정하고, 적은 수의 추가 파라미터만 학습하여 메모리와 계산량을 크게 줄일 수 있습니다.

## 2. 파라미터 설명

| 파라미터 | 설명 | 일반적인 값 |
|-----------|------|------------|
| r | 저랭크 행렬의 랭크 | 4, 8, 16 |
| alpha | LoRA scaling factor | 16, 32 |
| dropout | 드롭아웃 비율 | 0.05, 0.1 |
| target_modules | LoRA 적용 레이어 | ['q_proj', 'v_proj'] 등 |

## 3. 주요 모델에 대한 GPU 사양

| 모델명 | 모델 크기 | 최소 GPU 사양 | 권장 GPU 사양 |
|--------|-----------|--------------|--------------|
| Llama-7B | 7B | VRAM 16GB (3090) | VRAM 24GB (4090) |
| GPT-3 13B | 13B | VRAM 24GB (A6000) | VRAM 40GB (A100) |
| GPT-2 | 1.5B | VRAM 8GB (2080) | VRAM 16GB (3090) |

## 4. 핸즈온 Example

[이 섹션에는 실제 파인튜닝에 사용할 수 있는 코드 예제 또는 실습 예시를 추가합니다.]
