# Full Finetuning

## 1. 알고리즘 설명

Full Finetuning(전체 파인튜닝)은 사전학습된 모델의 모든 파라미터를 학습 데이터에 맞게 재학습하는 가장 기본적이고 강력한 파인튜닝 방식입니다. 일반적으로 대규모 데이터와 충분한 연산 자원이 있을 때 사용하며, 미세 조정 기법에 비해 더 많은 계산과 시간이 소요됩니다.

## 2. 파라미터 설명

| 파라미터 | 설명 | 일반적인 값 |
|-----------|------|-------------|
| learning_rate | 학습률 | 1e-5 ~ 5e-5 |
| batch_size | 배치 크기 | 8, 16, 32 |
| epochs | 학습 에폭 | 3, 5, 10 |
| weight_decay | 가중치 감쇠 | 0.01, 0.1 |

## 3. 주요 모델에 대한 GPU 사양

| 모델명 | 모델 크기 | 최소 GPU 사양 | 권장 GPU 사양 |
|--------|-----------|--------------|--------------|
| BERT-base | 110M | VRAM 4GB (2060) | VRAM 8GB (2080) |
| GPT-2 | 1.5B | VRAM 8GB (2080) | VRAM 16GB (3090) |
| Llama-7B | 7B | VRAM 24GB (A6000) | VRAM 40GB (A100) |

## 4. 핸즈온 Example

[이 섹션에는 실제 전체 파인튜닝에 사용할 수 있는 코드 예제 또는 실습 예시를 추가합니다.]
